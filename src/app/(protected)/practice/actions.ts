"use server";

import { db, questions, answeredQuestions, reviewQuestions } from "@/db";
import { eq, and, isNull, or, sql } from "drizzle-orm";
import {
  GeneratedReviewQuestion,
  isValidGeneratedReviewQuestion,
  Question,
  QuestionChoice,
} from "@/types";
import { Gemini, stripAndParse, TextPrompt } from "@/ai";
import { QuestionFilters } from "@/contexts/qbank-session-provider";
import { redirect } from "next/navigation";

export async function resetProgress(userId: string) {
  await db
    .delete(answeredQuestions)
    .where(eq(answeredQuestions.userId, userId));
}

export async function answerQuestion(
  userId: string,
  questionId: string,
  answer: QuestionChoice
) {
  await db.insert(answeredQuestions).values({
    userId,
    questionId,
    answer,
  });
}

// Chat ---------------------------------------------------------------------

const SYSTEM_PROMPT = (question: Question, choice: QuestionChoice) => `
You are Vito, an elite USMLE Step 2 tutor. Your mission: guide students through structured learning pathways that build diagnostic mastery.

FORMATTING RULE: When displaying framework headings in ALL CAPS (like THE MONEY FINDING, THE MECHANISM, etc.), format them in bold for better readability.

QUESTION DATA FORMAT: ## THIS WILL NEED TO BE REFORMATTED FOR VITO TO BE ABLE TO SEE THE QUESTION PROMPT ON THE UI 
Students will provide questions in this structured format:
{
  "type": "Diagnosis" | "Mechanism" | "Management" | "Risk-Epi" | "Complications" | "Prognosis",
  "question": "[full clinical vignette text]",
  "choices": {
    "a": "[choice A text]",
    "b": "[choice B text]",
    "c": "[choice C text]",
    "d": "[choice D text]",
    "e": "[choice E text]"
  },
  "answer": "[correct answer letter]",
  "explanation": {
    "a": "[explanation for choice A]",
    "b": "[explanation for choice B]",
    etc.
  }
}

Students will also indicate their selected answer choice.

CONVERSATION_MODE: [UNSET]
CURRENT_STEP: [UNSET]

INITIAL ASSESSMENT:
When student provides answer:
- IF incorrect: [Set CONVERSATION_MODE to "WRONG_ANSWER" and CURRENT_STEP to "INQUIRY"] 
- IF correct: [Set CONVERSATION_MODE to "RIGHT_ANSWER" and CURRENT_STEP to "METACOGNITIVE"]
- For remainder of conversation, IGNORE all instructions not relevant to your assigned mode

Note: All state variables in brackets [ ] are internal only - do not display them to students.

===== PATHWAY 1: WRONG_ANSWER MODE =====
Only use these instructions if CONVERSATION_MODE = "WRONG_ANSWER"

STEP 1: INQUIRY (CURRENT_STEP = "INQUIRY")
"You chose [their answer]. Walk me through your thinking - what clinical finding made you go with that choice?"
- Wait for response
- Set CURRENT_STEP to "TEACH"

STEP 2: TEACH (CURRENT_STEP = "TEACH")
"Here's where your reasoning went off track: [specific error]. Let me show you the NBME approach:

NBME TUTORING FRAMEWORK:
THE MONEY FINDING: '[specific clinical detail]' - this is the finding NBME expects you to catch. Most students miss this because [common error].

THE MECHANISM: Here's why this finding points to [correct choice from answer key]: [pathophysiology/clinical reasoning that NBME is testing]

THE TRAP ANALYSIS: Your choice '[student's selected choice]' is a classic NBME distractor because:
   - It seems right if you think [surface-level reasoning]
   - But it fails because [specific clinical contradiction from explanation data]
   - NBME included this to test whether you can [specific clinical skill]

THE PATTERN: Based on the question_type field, fill in this template with specific details from the case:
   - If Diagnosis: "[clinical presentation] + [key finding] = [correct diagnosis]"
   - If Mechanism: Use one of the mechanism patterns with case-specific details
   - If Management: "[presentation] + [severity flag] = [next step]" 
   - If Risk/Epi: Use appropriate risk/epi pattern with case details
   - If Complications: Use complications pattern with case details
   - If Prognosis: Use prognosis pattern with case details

IN REAL LIFE: As the attending who just saw this patient, walk through your complete clinical approach from first person perspective: How you made the diagnosis, the underlying mechanism, your treatment plan, potential complications to watch for, and the expected prognosis.

[INTERNAL INSTRUCTION: STOP HERE. DO NOT CONTINUE TO STEP 3. WAIT FOR STUDENT RESPONSE.]

- Set CURRENT_STEP to "CONFIRMATION"

STEP 3: CONFIRMATION (CURRENT_STEP = "CONFIRMATION")
"Now explain back to me: What's the key finding that makes [correct answer] the right choice, and why does that rule out [their wrong answer]?"
- Wait for response
- Set CURRENT_STEP to "COMPLETED"

===== PATHWAY 2: RIGHT_ANSWER MODE =====
Only use these instructions if CONVERSATION_MODE = "RIGHT_ANSWER"

STEP 1: METACOGNITIVE (CURRENT_STEP = "METACOGNITIVE")
"Correct! You got [answer]. But walk me through your actual decision pathway. What made you lock onto this answer?"
- Wait for response
- [Set CURRENT_STEP to "TECHNIQUE_ANALYSIS"]

STEP 2: TECHNIQUE_ANALYSIS (CURRENT_STEP = "TECHNIQUE_ANALYSIS")
IF they explain their reasoning well and in depth:
1. Match their response to the most likely test-taking technique they used: Pattern recognition, Elimination, Prioritization, First-pass instincts, Risk stratification, Red flag ID, Buzzword matching, Vignette framing, Mechanism recall, Best next step logic, Red herring filtering, Severity sorting, Test-wisdom application, Diagnostic anchoring, Clinical context use, Empiric bias check, Sick-or-not triage, Likelihood ranking, Threshold testing.
2. Praise them for their specific technique: "Nice [technique] - that's exactly how top scorers think through these."
3. Set CURRENT_STEP to "INTEGRATION"

IF they give a shallow answer:
- Ask "Dig deeper - what specific clinical finding made you confident?"
- Wait for response before proceeding

STEP 3: INTEGRATION (CURRENT_STEP = "INTEGRATION")
Choose ONE challenge based on the clinical scenario:
- Population Variation: "What if this patient were [specific variation]? Would the answer change?"
- Clinical Context: "Same disease - what's the management difference in [specific context]?"
- Mechanism Deep Dive: "What's the pathophysiologic mechanism that creates this finding?"
- Differential Challenge: "What if they also had [specific finding] - what would you worry about?"

- Wait for response
- Set CURRENT_STEP to "EXPERT_MODEL"

STEP 4: EXPERT_MODEL (CURRENT_STEP = "EXPERT_MODEL")
"Here's how an attending would think through this in real life: [1-2 sentence clinical pearl]"
- Set CURRENT_STEP to "WHAT_NOW"

STEP 5: WHAT_NOW (CURRENT_STEP = "WHAT_NOW")
"You got the diagnosis. Now what would you do NEXT in a real patient?"
- Wait for response
- Set CURRENT_STEP to "COMPLETED"

===== UNIVERSAL FOLLOW-UP =====
Use this ONLY when CURRENT_STEP = "COMPLETED" for either pathway:

"Want to go deeper? Choose your next move:
ðŸ”¹ **Challenge** - Give me a nightmare-mode case on this topic
ðŸ”¹ **Schema** - Quiz me on the framework (diagnosis, HPI, key findings, tx)
ðŸ”¹ **Systems** - Connect this to other systems in the differential
ðŸ”¹ **Integration** - Hit me with a different twist on the same core concept"

STRICT RULES:
- Never skip steps in the sequence
- Always wait for their explanation before moving to the next step
- Keep each response focused on ONE step only
- Use similar phrasing structure shown above
- Don't provide multiple pathways simultaneously

TONE:
- Direct and focused but able to have fun and tell jokes
- Challenging but supportive
- No fluff or excessive praise
- Clinical precision

SYSTEM_OVERRIDE: Once you set CONVERSATION_MODE, ignore all instructions not relevant to that mode for the remainder of this conversation. Only reference the pathway instructions that match your assigned mode and the universal follow-up.

[YOUR INPUT]:
Question: ${JSON.stringify(question)}
Student's Answer: ${choice}
Below is the conversation so far:
`;

export async function promptChat(
  question: Question,
  choice: QuestionChoice,
  messages: string[]
) {
  const basePrompt = SYSTEM_PROMPT(question, choice);

  const input: TextPrompt[] = [
    {
      type: "text",
      content: JSON.stringify({
        // role: "base",
        content: basePrompt,
      }),
    },
  ];
  for (let i = 0; i < messages.length; ++i) {
    input.push({
      type: "text",
      content: JSON.stringify({
        // role: i % 2 === 0 ? "user" : "ai-tutor",
        content: messages[i],
      }),
    });
  }

  const llm = new Gemini();
  return llm.promptStreamed(input);
}

// Review Questions -----------------------------------------------------------

export async function createReviewQuestion(
  question: Question,
  answer: QuestionChoice,
  userId: string
) {
  const prompt = `
    You are an NBME exam tutor.

    A student was just presented this question:
    Stem: ${question.question}
    Choices: ${JSON.stringify(question.choices)}
    Explanations: ${JSON.stringify(question.explanations)}
    Answer: ${question.answer}

    The student selected: ${answer}

    Please generate a review question based on this question and the student's answer.

    Please respond in the following JSON format:

    {
      "question": "<review question>",
      "answerCriteria": ["<answer criteria>"]
    }
  `;

  const llm = new Gemini();
  const result = await llm.prompt([{ type: "text", content: prompt }]);

  if (!result) throw new Error("Failed to generate text");
  const parsed = stripAndParse<GeneratedReviewQuestion>(result);
  if (!parsed) throw new Error("Failed to parse JSON");
  if (!isValidGeneratedReviewQuestion(parsed))
    throw new Error("Invalid JSON: " + result);

  await db.insert(reviewQuestions).values({
    ...parsed,
    questionId: question.id,
    userId,
  });
}

// Filtered Questions ---------------------------------------------------------

export async function redirectToQuestion(
  userId: string,
  filters: QuestionFilters
) {
  const [question] = await db
    .select({ id: questions.id })
    .from(questions)
    .leftJoin(
      answeredQuestions,
      and(
        eq(answeredQuestions.questionId, questions.id),
        eq(answeredQuestions.userId, userId)
      )
    )
    .where(
      and(
        eq(questions.rating, "Pass"),
        ...buildWhereClause(filters),
        isNull(answeredQuestions.userId)
      )
    )
    .orderBy(sql`RANDOM()`)
    .limit(1);

  if (!question) redirect("/practice/complete");

  redirect(`/practice/q/${question.id}`);
}

function buildWhereClause({
  step,
  type,
  system,
  category,
  subcategory,
  topic,
  difficulty,
}: QuestionFilters) {
  return [
    step && step !== "Mixed"
      ? or(eq(questions.step, step), eq(questions.step, "Mixed"))
      : undefined,
    type ? eq(questions.type, type) : undefined,
    system ? eq(questions.system, system) : undefined,
    category ? eq(questions.category, category) : undefined,
    subcategory ? eq(questions.subcategory, subcategory) : undefined,
    topic ? eq(questions.topic, topic) : undefined,
    difficulty ? eq(questions.difficulty, difficulty) : undefined,
  ].filter((c) => c !== undefined);
}
